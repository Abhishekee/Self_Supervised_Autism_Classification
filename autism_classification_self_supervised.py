# -*- coding: utf-8 -*-
"""Autism_Classification_Self_Supervised.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hR3saO5_0BZhN2sX0K-od4vtlyllYQ93
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install pytorch-lightning
!pip install lightning-bolts
!pip install torchmetrics

pip install git+https://github.com/PytorchLightning/lightning-bolts.git@master --upgrade

import pandas as pd
import matplotlib.pyplot as plt

import os
import torch
import pl_bolts
import torchvision
import torchmetrics
import pytorch_lightning as pl

from torch.optim import Adam
from torch.nn.functional import cross_entropy
from torchmetrics.functional import accuracy

from torchvision import models, transforms
from torchvision.utils import make_grid
from torch.utils.data import Dataset, random_split
from torchvision.datasets import ImageFolder
from torch.utils.data.dataloader import DataLoader

from pytorch_lightning import Callback
from pytorch_lightning.loggers import CSVLogger
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping

data_dir = '/content/drive/MyDrive/ABIDE Dataset/ABIDE I'
os.listdir(data_dir)

basic_transformer = torchvision.transforms.Compose(
    [
        torchvision.transforms.Resize((224, 224)),
     torchvision.transforms.ToTensor(),
     torchvision.transforms.Normalize(
         mean = [0.485, 0.485, 0.406], std = [0.229, 0.224, 0.225]
     )
    ]
)

asd_dataset = ImageFolder(data_dir, transform = basic_transformer)
len(asd_dataset)

asd_dataset.classes

def show_img_example(img, label):
  print('Label: ', asd_dataset.classes[label], "(" + str(label) + ")")
  plt.imshow(img.permute(1, 2, 0))

show_img_example(*asd_dataset[5])

show_img_example(*asd_dataset[25422])

class AutismDataModule(pl.LightningDataModule):
   def __init__(self, data_dir = '/content/drive/MyDrive/ABIDE Dataset/ABIDE I' ):
       super().__init__()

       self.data_dir = data_dir
       self.transform = transforms.Compose(
        [
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(p = 0.5),
            transforms.RandomVerticalFlip(p = 0.1),
            transforms.RandomRotation(10),
               transforms.ToTensor(),
               transforms.Normalize(
                   mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225)
               ),

        ]
      )

   def setup(self, stage=None):
      asd_full = ImageFolder(self.data_dir, transform = self.transform)
      length = len(asd_full)
      val_split_size = 0.25
      train_length = round(length * (1- val_split_size))
      val_length = length - train_length

      self.asd_train, self.asd_val = \
        random_split(asd_full, [train_length, val_length])

   def train_dataloader(self):
       train_dataloader = \
       DataLoader(self.asd_train, batch_size = 32, shuffle = True)

       return train_dataloader

   def val_dataloader(self):
       val_dataloader = \
       DataLoader(self.asd_val, batch_size = 32)

       return val_dataloader

asd_dm = AutismDataModule()
asd_dm.setup()

train_dl = asd_dm.train_dataloader()
len(train_dl)

valid_dl = asd_dm.val_dataloader()
len(valid_dl)

for inp, label in train_dl:
  print('{} : {}'.format(inp, label))
  print('Input shape: ', inp.shape)
  print('Labels shape: ', label.shape)
  break

def show_batch_images(dl):
  for images, labels in dl:
    fig, ax = plt.subplots(figsize = (12, 6))
    ax.imshow(make_grid(images, nrow = 8).permute(1, 2, 0))
    break

show_batch_images(train_dl)

"""#RESNET-18 MODEL (TRANSFER LEARNING)"""

from torch.nn.modules import loss
class AutismClassifier(pl.LightningModule):
  def __init__(self, num_classes = 2, lr = 1e-3):
      super().__init__()
      self.save_hyperparameters()

      self.backbone = models.resnet18(pretrained = True)

      self.finetune_layer = torch.nn.Linear(self.backbone.fc.out_features, num_classes)

  def training_step(self, batch, batch_idx):

      x, y = batch
      

      with torch.no_grad():
           features = self.backbone(x)

      preds = self.finetune_layer(features)
      

      loss = cross_entropy(preds, y)
     

      metrics = {'train_loss': loss}
      self.log_dict(metrics, on_step= False, on_epoch= True,
                    prog_bar = True, logger = True)
      
      return loss


  def validation_step(self, batch, bach_idx):

      x, y = batch
      

      features = self.backbone(x)
      preds = self.finetune_layer(features)
      
      loss = cross_entropy(preds, y)
      

      metrics = {'valid_loss': loss}
      self.log_dict(metrics, on_step = False, on_epoch= True,
                    prog_bar = True, logger = True)
      
      return metrics


  def configure_optimizers(self):

    optimizer = Adam(self.parameters(), lr = self.hparams.lr)
 
    return optimizer



logger = CSVLogger(save_dir = './lightning_logs', name = 'resnet18_bb_frozen')

from pytorch_lightning.trainer import trainer
classifier = AutismClassifier()

trainer = pl.Trainer(max_epochs = 20, 
                     limit_train_batches=20, 
                     log_every_n_steps= 10, 
                     logger = logger, gpus = 1)

trainer.fit(classifier, asd_dm)

metrics = pd.read_csv(f'{trainer.logger.log_dir}/metrics.csv')

metrics.head()

train_loss = metrics['train_loss'].dropna().reset_index(drop = True)
valid_loss = metrics['valid_loss'].dropna().reset_index(drop = True)

fig = plt.figure(figsize = (12, 6))
plt.grid(True)

plt.plot(train_loss, color = 'r', marker = 'o', label = 'train_loss')
plt.plot(valid_loss, color = 'b', marker = 'x', label = 'valid_loss')

plt.ylabel('Loss', fontsize = 24)
plt.xlabel('Epoch', fontsize = 24)

plt.legend(loc = 'upper right', fontsize = 18)
plt.savefig(f'{trainer.logger.log_dir}/loss_resnet18.png')

from torch.nn.modules import loss
class AutismClassifier(pl.LightningModule):
  def __init__(self, num_classes = 2, lr = 1e-3):
      super().__init__()
      self.save_hyperparameters()

      self.backbone = models.resnet18(pretrained = True)

      self.finetune_layer = torch.nn.Linear(self.backbone.fc.out_features, num_classes)

  def training_step(self, batch, batch_idx):

      x, y = batch
      

      if self.trainer.current_epoch < 10:
            with torch.no_grad():
                features = self.backbone(x)
      else:
             features = self.backbone(x)
        
      preds = self.finetune_layer(features)

      loss = cross_entropy(preds, y)
     

      metrics = {'train_loss': loss}
      self.log_dict(metrics, on_step = False, on_epoch = True, 
                      prog_bar = True, logger = True)
        
      return loss
    


  def validation_step(self, batch, bach_idx):

      x, y = batch
      

      features = self.backbone(x)
      preds = self.finetune_layer(features)
      
      loss = cross_entropy(preds, y)
      

      metrics = {'valid_loss': loss}
      self.log_dict(metrics, on_step = False, on_epoch= True,
                    prog_bar = True, logger = True)
      
      return metrics


  def configure_optimizers(self):

    optimizer = Adam(self.parameters(), lr = self.hparams.lr)
 
    return optimizer

logger = CSVLogger(save_dir = './lightning_logs', name = 'resnet18_bb_unfrozen')

classifier = AutismClassifier()

trainer = pl.Trainer(limit_train_batches = 20, max_epochs = 20, log_every_n_steps = 10,
                     logger = logger, gpus = 1)

trainer.fit(classifier, asd_dm)

metrics = pd.read_csv(f'{trainer.logger.log_dir}/metrics.csv')

metrics.head()

train_loss = metrics['train_loss'].dropna().reset_index(drop = True)
valid_loss = metrics['valid_loss'].dropna().reset_index(drop = True)

fig = plt.figure(figsize = (12, 6))
plt.grid(True)

plt.plot(train_loss, color = 'r', marker = 'o', label = 'train/loss')
plt.plot(valid_loss, color = 'b', marker = 'x', label = 'valid/loss')

plt.ylabel('Loss', fontsize = 24)
plt.xlabel('Epoch', fontsize = 24)

plt.legend(loc = 'upper right', fontsize = 18)
plt.savefig(f'{trainer.logger.log_dir}/loss_resnet18_unfrozen.png')

"""#VGG16 MODEL (TRANSFER LEARNING)"""

from torch.nn.modules import loss
class AutismClassifier(pl.LightningModule):
  def __init__(self, num_classes = 2, lr = 1e-3):
      super().__init__()
      self.save_hyperparameters()

      self.backbone = models.vgg16_bn(pretrained = True)

      self.finetune_layer = torch.nn.Linear(self.backbone.classifier[6].out_features, num_classes)

  def training_step(self, batch, batch_idx):

      x, y = batch
      

      with torch.no_grad():
           features = self.backbone(x)

      preds = self.finetune_layer(features)
      

      loss = cross_entropy(preds, y)
      # accuracy_score = torchmetrics.functional.accuracy(preds, y)

      metrics = {'train_loss': loss}
      self.log_dict(metrics, on_step= False, on_epoch= True,
                    prog_bar = True, logger = True)
      
      return loss


  def validation_step(self, batch, bach_idx):

      x, y = batch
      

      features = self.backbone(x)
      preds = self.finetune_layer(features)
      
      loss = cross_entropy(preds, y)
      # accuracy_score = torchmetrics.functional.accuracy(preds, y, 'binary')

      metrics = {'valid_loss': loss}
      self.log_dict(metrics, on_step = False, on_epoch= True,
                    prog_bar = True, logger = True)
      
      return metrics


  def configure_optimizers(self):

    optimizer = Adam(self.parameters(), lr = self.hparams.lr)
 
    return optimizer

logger = CSVLogger(save_dir = './lightning_logs', name = 'vgg16_bb_unfrozen')



classifier = AutismClassifier()

trainer = pl.Trainer(limit_train_batches = 20, max_epochs = 20, log_every_n_steps = 10,
                     logger = logger, gpus = 1)

trainer.fit(classifier, asd_dm)

metrics = pd.read_csv(f'{trainer.logger.log_dir}/metrics.csv')

metrics.head()

train_loss = metrics['train_loss'].dropna().reset_index(drop = True)
valid_loss = metrics['valid_loss'].dropna().reset_index(drop = True)

fig = plt.figure(figsize = (12, 6))
plt.grid(True)

plt.plot(train_loss, color = 'r', marker = 'o', label = 'train/loss')
plt.plot(valid_loss, color = 'b', marker = 'x', label = 'valid/loss')

plt.ylabel('Loss', fontsize = 24)
plt.xlabel('Epoch', fontsize = 24)

plt.legend(loc = 'upper right', fontsize = 18)
plt.savefig(f'{trainer.logger.log_dir}/loss_VGG16.png')

from torch.nn.modules import loss
class AutismClassifier(pl.LightningModule):
  def __init__(self, num_classes = 2, lr = 1e-3):
      super().__init__()
      self.save_hyperparameters()

      self.backbone = models.vgg16_bn(pretrained = True)

      self.finetune_layer = torch.nn.Linear(self.backbone.classifier[6].out_features, num_classes)

  def training_step(self, batch, batch_idx):

      x, y = batch
      

      if self.trainer.current_epoch < 10:
            with torch.no_grad():
                features = self.backbone(x)
      else:
             features = self.backbone(x)
        
      preds = self.finetune_layer(features)

      loss = cross_entropy(preds, y)
     

      metrics = {'train_loss': loss}
      self.log_dict(metrics, on_step = False, on_epoch = True, 
                      prog_bar = True, logger = True)
        
      return loss


  def validation_step(self, batch, bach_idx):

      x, y = batch
      

      features = self.backbone(x)
      preds = self.finetune_layer(features)
      
      loss = cross_entropy(preds, y)
      # accuracy_score = torchmetrics.functional.accuracy(preds, y, 'binary')

      metrics = {'valid_loss': loss}
      self.log_dict(metrics, on_step = False, on_epoch= True,
                    prog_bar = True, logger = True)
      
      return metrics


  def configure_optimizers(self):

    optimizer = Adam(self.parameters(), lr = self.hparams.lr)
 
    return optimizer

logger = CSVLogger(save_dir = './lightning_logs', name = 'vgg16_bb_no_frozen')



classifier = AutismClassifier()

trainer = pl.Trainer(limit_train_batches = 20, max_epochs = 20, log_every_n_steps = 10,
                     logger = logger, gpus = 1)

trainer.fit(classifier, asd_dm)

metrics = pd.read_csv(f'{trainer.logger.log_dir}/metrics.csv')

metrics.head()

train_loss = metrics['train_loss'].dropna().reset_index(drop = True)
valid_loss = metrics['valid_loss'].dropna().reset_index(drop = True)

fig = plt.figure(figsize = (12, 6))
plt.grid(True)

plt.plot(train_loss, color = 'r', marker = 'o', label = 'train/loss')
plt.plot(valid_loss, color = 'b', marker = 'x', label = 'valid/loss')

plt.ylabel('Loss', fontsize = 24)
plt.xlabel('Epoch', fontsize = 24)

plt.legend(loc = 'upper right', fontsize = 18)
plt.savefig(f'{trainer.logger.log_dir}/loss_VGG16_unfrozen.png')

"""#AlexNet Model (Transfer Learning)"""

from torch.nn.modules import loss
class AutismClassifier(pl.LightningModule):
  def __init__(self, num_classes = 2, lr = 1e-3):
      super().__init__()
      self.save_hyperparameters()

      self.backbone = models.alexnet(pretrained = True)

      self.finetune_layer = torch.nn.Linear(self.backbone.classifier[6].out_features, num_classes)

  def training_step(self, batch, batch_idx):

      x, y = batch
      

      if self.trainer.current_epoch < 10:
            with torch.no_grad():
                features = self.backbone(x)
      else:
             features = self.backbone(x)
        
      preds = self.finetune_layer(features)

      loss = cross_entropy(preds, y)
     

      metrics = {'train_loss': loss}
      self.log_dict(metrics, on_step = False, on_epoch = True, 
                      prog_bar = True, logger = True)
        
      return loss


  def validation_step(self, batch, bach_idx):

      x, y = batch
      

      features = self.backbone(x)
      preds = self.finetune_layer(features)
      
      loss = cross_entropy(preds, y)
      # accuracy_score = torchmetrics.functional.accuracy(preds, y, 'binary')

      metrics = {'valid_loss': loss}
      self.log_dict(metrics, on_step = False, on_epoch= True,
                    prog_bar = True, logger = True)
      
      return metrics


  def configure_optimizers(self):

    optimizer = Adam(self.parameters(), lr = self.hparams.lr)
 
    return optimizer

logger = CSVLogger(save_dir = './lightning_logs', name = 'Alexnet_bb_no_frozen')

classifier = AutismClassifier()

trainer = pl.Trainer(limit_train_batches = 20, max_epochs = 20, log_every_n_steps = 10,
                     logger = logger, gpus = 1)

trainer.fit(classifier, asd_dm)

metrics = pd.read_csv(f'{trainer.logger.log_dir}/metrics.csv')

metrics.head()

train_loss = metrics['train_loss'].dropna().reset_index(drop = True)
valid_loss = metrics['valid_loss'].dropna().reset_index(drop = True)

fig = plt.figure(figsize = (12, 6))
plt.grid(True)

plt.plot(train_loss, color = 'r', marker = 'o', label = 'train/loss')
plt.plot(valid_loss, color = 'b', marker = 'x', label = 'valid/loss')

plt.ylabel('Loss', fontsize = 24)
plt.xlabel('Epoch', fontsize = 24)

plt.legend(loc = 'upper right', fontsize = 18)
plt.savefig(f'{trainer.logger.log_dir}/loss_Alexnet_unfrozen.png')

from torch.nn.modules import loss
class AutismClassifier(pl.LightningModule):
  def __init__(self, num_classes = 2, lr = 1e-3):
      super().__init__()
      self.save_hyperparameters()

      self.backbone = models.alexnet(pretrained = True)

      self.finetune_layer = torch.nn.Linear(self.backbone.classifier[6].out_features, num_classes)

  def training_step(self, batch, batch_idx):

      x, y = batch
      

      if self.trainer.current_epoch < 10:
            with torch.no_grad():
                features = self.backbone(x)
      else:
             features = self.backbone(x)
        
      preds = self.finetune_layer(features)

      loss = cross_entropy(preds, y)
     

      metrics = {'train_loss': loss}
      self.log_dict(metrics, on_step = False, on_epoch = True, 
                      prog_bar = True, logger = True)
        
      return loss


  def validation_step(self, batch, bach_idx):

      x, y = batch
      

      features = self.backbone(x)
      preds = self.finetune_layer(features)
      
      loss = cross_entropy(preds, y)
      # accuracy_score = torchmetrics.functional.accuracy(preds, y, 'binary')

      metrics = {'valid_loss': loss}
      self.log_dict(metrics, on_step = False, on_epoch= True,
                    prog_bar = True, logger = True)
      
      return metrics


  def configure_optimizers(self):

    optimizer = Adam(self.parameters(), lr = self.hparams.lr)
 
    return optimizer

logger = CSVLogger(save_dir = './lightning_logs', name = 'Alexnet_bb_no_frozen')

"""# Self Supervised Learning

##Contrastive Learning : SimCLR
"""

from pl_bolts.models.self_supervised import SimCLR

class AutismClassifier(pl.LightningModule):
    def __init__(self, num_classes = 2, lr = 1e-3):
        super().__init__()
        self.save_hyperparameters()
        
        weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/simclr/bolts_simclr_imagenet/simclr_imagenet.ckpt'

        simclr = SimCLR.load_from_checkpoint(weight_path, strict = False)

        self.backbone = simclr
        self.finetune_layer = torch.nn.Linear(2048, num_classes)
        
    def training_step(self, batch, batch_idx):
        x, y = batch

        if self.trainer.current_epoch < 10:
            with torch.no_grad():
                features = self.backbone(x)
        else:
            features = self.backbone(x)

        preds = self.finetune_layer(features)

        loss = cross_entropy(preds, y)
       

        metrics = {'train_loss': loss}
        self.log_dict(metrics, on_step = False, on_epoch = True, 
                      prog_bar = True, logger = True)
        
        return loss

    def validation_step(self, batch, batch_idx):
       
        x, y = batch   

        features = self.backbone(x)

        preds = self.finetune_layer(features)

        loss = cross_entropy(preds, y)
        

        metrics = {'valid_loss': loss}
        self.log_dict(metrics, on_step = False, on_epoch = True, 
                      prog_bar = True, logger = True)
        
        return metrics

    def configure_optimizers(self):
        
        optimizer = Adam(self.parameters(), lr = self.hparams.lr)
        
        return optimizer

logger = CSVLogger(save_dir = './lightning_logs', name = 'simclr_bb_unfrozen')

classifier = AutismClassifier()

trainer = pl.Trainer(max_epochs = 20,
                     limit_train_batches = 20, 
                     log_every_n_steps = 10, logger = logger, gpus = 1)

trainer.fit(classifier, asd_dm)

metrics = pd.read_csv(f'{trainer.logger.log_dir}/metrics.csv')

metrics.head()

train_loss = metrics['train_loss'].dropna().reset_index(drop = True)
valid_loss = metrics['valid_loss'].dropna().reset_index(drop = True)

fig = plt.figure(figsize = (12, 6))
plt.grid(True)

plt.plot(train_loss, color = 'r', marker = 'o', label = 'train_loss')
plt.plot(valid_loss, color = 'b', marker = 'x', label = 'valid_loss')

plt.ylabel('Loss', fontsize = 24)
plt.xlabel('Epoch', fontsize = 24)

plt.legend(loc = 'upper right', fontsize = 18)
plt.savefig(f'{trainer.logger.log_dir}/loss_SimCLR.png')

"""#Non Contrastive Learning: SwAV"""

from pl_bolts.models.self_supervised import SwAV

class AutismClassifier(pl.LightningModule):
    def __init__(self, num_classes = 2, lr = 1e-3):
        super().__init__()
        self.save_hyperparameters()

        weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/swav/swav_imagenet/swav_imagenet.pth.tar'
        swav = SwAV.load_from_checkpoint(weight_path, strict = True)

        self.backbone = swav.model
        self.finetune_layer = torch.nn.Linear(3000, num_classes)
        
    def training_step(self, batch, batch_idx):
        x, y = batch

        if self.trainer.current_epoch < 10:
            with torch.no_grad():
                  features = self.backbone(x)[-1]
        else:
            features = self.backbone(x)[-1]

        preds = self.finetune_layer(features)

        loss = cross_entropy(preds, y)
       

        metrics = {'train_loss': loss}
        self.log_dict(metrics, on_step = False, on_epoch = True, 
                      prog_bar = True, logger = True)
        
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch   

        features = self.backbone(x)[-1]
        preds = self.finetune_layer(features)

        loss = cross_entropy(preds, y)
        

        metrics = {'valid_loss': loss}
        self.log_dict(metrics, on_step = False, on_epoch = True, 
                      prog_bar = True, logger = True)
        
        return metrics

    def configure_optimizers(self):
        
        optimizer = Adam(self.parameters(), lr = self.hparams.lr)

        return optimizer

logger = CSVLogger(save_dir = './lightning_logs', name = 'swav_bb_unfrozen')

classifier = AutismClassifier()

trainer = pl.Trainer(max_epochs = 20,
                     limit_train_batches = 20, 
                     log_every_n_steps = 10, logger = logger, gpus = 1)

trainer.fit(classifier, asd_dm)

metrics = pd.read_csv(f'{trainer.logger.log_dir}/metrics.csv')

metrics.head()

train_loss = metrics['train_loss'].dropna().reset_index(drop = True)
valid_loss = metrics['valid_loss'].dropna().reset_index(drop = True)

fig = plt.figure(figsize = (12, 6))
plt.grid(True)

plt.plot(train_loss, color = 'r', marker = 'o', label = 'train_loss')
plt.plot(valid_loss, color = 'b', marker = 'x', label = 'valid_loss')

plt.ylabel('Loss', fontsize = 24)
plt.xlabel('Epoch', fontsize = 24)

plt.legend(loc = 'upper right', fontsize = 18)
plt.savefig(f'{trainer.logger.log_dir}/loss_SwAV.png')